# ETL (Extract, Transform, Load)

## Определение
**ETL** — это процесс извлечения данных из источников, их преобразования в нужный формат и загрузки в целевую систему (например, Data Warehouse).

## Зачем нужен ETL?
- **Консолидация данных** из разных источников
- **Очистка и стандартизация** данных
- **Подготовка данных** для анализа и ML
- **Создание единой версии правды**

## Три этапа ETL

### 1. Extract (Извлечение)
Получение данных из различных источников:

#### Источники данных:
- **Базы данных** (MySQL, PostgreSQL, MongoDB)
- **Файлы** (CSV, JSON, XML, Excel)
- **API** (REST, SOAP, GraphQL)
- **Потоки данных** (Kafka, RabbitMQ)
- **Веб-скрапинг**

#### Методы извлечения:
- **Full Load** - полная выгрузка всех данных
- **Incremental Load** - только новые/измененные данные
- **CDC (Change Data Capture)** - отслеживание изменений в реальном времени

### 2. Transform (Преобразование)
Обработка и очистка данных:

#### Основные преобразования:
- **Очистка данных**:
  - Удаление дубликатов
  - Исправление опечаток
  - Заполнение пропущенных значений
- **Стандартизация**:
  - Приведение форматов (даты, валюты)
  - Нормализация текста
- **Обогащение**:
  - Добавление вычисляемых полей
  - Объединение с справочными данными
- **Агрегация**:
  - Суммирование, усреднение
  - Группировка данных

### 3. Load (Загрузка)
Запись обработанных данных в целевую систему:

#### Целевые системы:
- **Data Warehouses** (Redshift, BigQuery, Snowflake)
- **Data Lakes** (S3, ADLS)
- **OLAP системы** (ClickHouse, Druid)
- **Базы данных**

#### Стратегии загрузки:
- **Full Refresh** - полная перезапись
- **Incremental** - добавление новых данных
- **Upsert** - обновление существующих + добавление новых

## ETL vs ELT

### ETL (Traditional)
Источники -> Extract -> Transform -> Load -> Data Warehouse

- **Преобразования** до загрузки в DWH
- **Плюсы**: чистота данных, безопасность
- **Минусы**: ограниченная масштабируемость

### ELT (Modern)
Источники → Extract → Load → Transform → Data Warehouse

- **Преобразования** после загрузки в DWH
- **Плюсы**: гибкость, использование мощности DWH
- **Минусы**: требует больше места, сложнее управление

## Архитектура ETL пайплайна

### Компоненты:
1. **Orchestrator** - управление workflow (Airflow, Dagster)
2. **Processing Engine** - обработка данных (Spark, Pandas)
3. **Storage** - временное хранение (S3, HDFS)
4. **Monitoring** - мониторинг выполнения
5. **Error Handling** - обработка ошибок

## Data Quality в ETL

### Проверки качества данных:
- **Completeness** - полнота данных
- **Accuracy** - точность и корректность
- **Consistency** - согласованность между источниками
- **Timeliness** - актуальность данных
- **Validity** - соответствие бизнес-правилам

### Метрики качества:
- % пропущенных значений
- % некорректных форматов
- Количество дубликатов
- Время обработки данных

## Лучшая практика

### 1. Идемпотентность
Пайплайн должен давать одинаковый результат при многократном запуске.

### 2. Мониторинг и логирование
- Треккинг выполнения задач
- Алерты при ошибках
- Метрики производительности

### 3. Обработка ошибок
- Retry механизмы
- Уведомления админам

### 4. Масштабируемость
- Горизонтальное масштабирование
- Разделение на микросервисы
- Использование облачных ресурсов

## ETL в Data Science

### Для чего нужно DS:
1. **Feature Store** - подготовка признаков для ML
2. **Data Preparation** - очистка данных для обучения моделей
3. **A/B Testing** - сбор и обработка экспериментальных данных
4. **Monitoring** - треккинг дрейфа данных и метрик моделей