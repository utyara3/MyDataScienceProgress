# Энтропия

## Определения
Энтропия (информационная энтропия Шеннона) - мера неопределенности или хаоса в системе. Чем выше энтропия, тем менее предсказуемы выходные данные.

## Формула
Для дискретного распределения с вероятностями - `p₁, p₂, ..., pₙ`:

`H(X) = - Σ (pᵢ * log₂(pᵢ))`

где:
- `H(X)` — энтропия случайной величины X.
- `pᵢ` — вероятность i-го исхода.
- `Σ` — сумма по всем возможным исходам.

## Пример
Например, у нас есть корзина с фруктами. Рассмотрим разные случаи:
- В корзине лежат только яблоки.
- В корзине лежат поровну яблоки и апельсины.
- В корзине 7 видов фрукт и их количество разное

Рассмотрим энтропию для этих случаев, поставив такой вопрос: "С какой вероятностью при попытке достать фрукт из корзины я достану яблоко".

Для первого случая вероятность будет 100% и энтропия будет низкой.
Для второго случая вероятность будет 50/50. Вероятность *угадать*, *предсказать* стала ниже. Энтропия повысилась.
Для третьего случая вероятность будет очень низкой, по сравнению с первым случаем. Энтропия высокая.

## Вывод
Энтропия - мера неопределенности, хаоса или информационной насыщенности системы. Чем выше энтропия, тем менее предсказуемо состояние системы и тем больше информации мы получаем, узнав результат (например, в нашем случае - вытащив фрукт).

## Применение в Data Science

- Машинное обучение
Разделение и распределение данных. Алгоритм должен решить: "По какому признаку разделить данные, чтобы максимально возможно понизить энтропию"
Цель: Получить чистые (низкоэнтропийные) группы. Например, в одной группе будут клиенты которые вероятнее всего купять товар, а в другой - которые не купят.

- NLP
Энтропия текста может показывать его разнообразие.

